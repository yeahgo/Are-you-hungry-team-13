{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the key features we scrape from the Yelp website:\n",
    "1. Restaurant name\n",
    "2. Restaurant address\n",
    "3. Restaurant rating (yelp rating 0-5 with 0.5 increment)\n",
    "4. Hygiene (official scores: A, B, C)\n",
    "5. Restaurant neighborhood (Morningside height, East Village, Chelsea, etc.)\n",
    "6. Category (i.e. cuisine type: Chinese, Japanese, French, American, etc.)\n",
    "7. Noise Level (Quiet, Noisy, Average)\n",
    "8. Ambience (Romantic, trendy, etc.)\n",
    "9. Price range (under  $$10, $11-30, $31-60, ... )\n",
    "10. Parking Options (Street, Private Lot, Garage, etc)\n",
    "11. Reservable? (Yes, No)\n",
    "12. Has Gluten-free Options (Yes, No)\n",
    "13. Alcohol (Beer & Wine Only, Full Bar, None)\n",
    "\n",
    "     $ \\vdots$\n",
    "     \n",
    "We ran the scraping code for the following cuisine types: Chinese, Korean, American, Indian, Japanese, Spanish, French, Italian, Greek, Thai, Mexico, Vietnamese. (change the first argument of the \"get_urls_from_search(term, location, num)\" function to get urls for each cuisine type)\n",
    "\n",
    "For each cuisine type, we generate \"{cuisine_type}_Restaurant.csv\"\n",
    "\n",
    "**Use the following code to scrape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from threading import Thread\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import time\n",
    "from random import randint\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opener = urllib.request.build_opener()\n",
    "# IE 9 proved to be the most successful\n",
    "opener.addheaders = [('User-agent', 'IE 9/Windows: Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that will do the scraping job from yelp\n",
    "def scrape(ur):\n",
    "\n",
    "    with urllib.request.urlopen(ur) as url:\n",
    "        html = url.read()\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    retaurant_name = soup.find('h1')\n",
    "    \n",
    "    # create a dictionary business info for storing key business features \n",
    "    business_info = {}\n",
    "    business_info['restaurant_name']= str(retaurant_name.text.strip().rstrip())\n",
    "    \n",
    "    if soup.find('span',itemprop=\"streetAddress\") != None:\n",
    "        retaurant_address = soup.find('span',itemprop=\"streetAddress\")\n",
    "        business_info['retaurant_address'] = str(retaurant_address.text.strip().rstrip())\n",
    "    \n",
    "    if soup.find('span',itemprop=\"postalCode\") != None:\n",
    "        restaurant_zipcode = soup.find('span',itemprop=\"postalCode\")\n",
    "        business_info['restaurant_zipcode'] = str(restaurant_zipcode.text.strip().rstrip())\n",
    "    \n",
    "    if soup.find('span',itemprop=\"reviewCount\") != None:\n",
    "        restaurant_reviewcount = soup.find('span',itemprop=\"reviewCount\")\n",
    "        business_info['restaurant_reviewcount'] = str(restaurant_reviewcount.text.strip().rstrip())\n",
    "   \n",
    "    if soup.find(itemprop=\"ratingValue\") != None:\n",
    "        business_info['restaurant_rating'] = soup.find(itemprop=\"ratingValue\").get(\"content\")\n",
    "\n",
    "    if soup.find('span', {'class': 'neighborhood-str-list'}) != None:\n",
    "        neighborhood = soup.find('span', {'class': 'neighborhood-str-list'})\n",
    "        business_info['restaurant_neighobrhood'] = str(neighborhood.text.strip().rstrip())\n",
    "   \n",
    "    if soup.find('dd',{'class':\"nowrap health-score-description\"}) != None:\n",
    "        hygiene_score = soup.find('dd',{'class':\"nowrap health-score-description\"})\n",
    "        business_info['Hygiene_score'] = str(hygiene_score.text.strip().rstrip())\n",
    "        \n",
    "    if soup.find('dd', {'class':\"nowrap price-description\"}) != None:\n",
    "        price_range = soup.find('dd', {'class':\"nowrap price-description\"})\n",
    "        business_info['price_range'] = str(price_range.text.strip().rstrip())\n",
    "   \n",
    "    if soup.find('div',{'class':'short-def-list'}) != None:\n",
    "        for i in soup.find('div',{'class':'short-def-list'}).findAll('dl'):\n",
    "            key = i.find('dt').text.strip().rstrip()\n",
    "            value = i.find('dd').text.strip().rstrip()\n",
    "            business_info[str(key)]=str(value)\n",
    "    \n",
    "    if soup.find(property=\"place:location:latitude\") != None:\n",
    "        business_info['latitude'] = soup.find(property=\"place:location:latitude\").get(\"content\")\n",
    "\n",
    "    if soup.find(property=\"place:location:longitude\") != None:\n",
    "        business_info['longitude'] = soup.find(property=\"place:location:longitude\").get(\"content\")  \n",
    "    \n",
    "    business_info['Category']= ''\n",
    "    if soup.find('span',{'class':'category-str-list'}) != None:\n",
    "        for i in soup.find('span',{'class':'category-str-list'}).findAll('a'):\n",
    "            business_info['Category'] += (str(i.text.strip().rstrip())+'; ')\n",
    "                \n",
    "    return business_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'restaurant_name': 'Mount Everest Indiaâ€™s Cuisine', 'retaurant_address': '3641 W Sahara Ave', 'restaurant_zipcode': '89102', 'restaurant_reviewcount': '1442', 'restaurant_rating': '4.5', 'restaurant_neighobrhood': 'Westside', 'price_range': '$11-30', 'Has Soy-free Options': 'Yes', 'Has Gluten-free Options': 'Yes', 'Liked by Vegetarians': 'Yes', 'Has Dairy-free Options': 'Yes', 'Has Halal Options': 'Yes', 'Liked by Vegans': 'Yes', 'Takes Reservations': 'Yes', 'Delivery': 'Yes', 'Take-out': 'Yes', 'Accepts Credit Cards': 'Yes', 'Accepts Apple Pay': 'No', 'Accepts Google Pay': 'No', 'Good For': 'Lunch, Dinner', 'Parking': 'Private Lot', 'Bike Parking': 'Yes', 'Wheelchair Accessible': 'Yes', 'Good for Kids': 'Yes', 'Good for Groups': 'Yes', 'Attire': 'Casual', 'Ambience': 'Casual', 'Noise Level': 'Average', 'Alcohol': 'Beer & Wine Only', 'Outdoor Seating': 'No', 'Wi-Fi': 'No', 'Has TV': 'Yes', 'Dogs Allowed': 'No', 'Waiter Service': 'Yes', 'Caters': 'Yes', 'Category': 'Indian; '}\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "d = scrape('https://www.yelp.com/biz/mount-everest-indias-cuisine-las-vegas?osq=indian+food')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of yelp urls to scrape\n",
    "# change term to change the keyword we want to search\n",
    "def get_urls_from_search(term, location, num):\n",
    "    \n",
    "    term = term.replace(' ','+')\n",
    "    location = location.replace(' ','+')\n",
    "    query = 'https://www.yelp.com/search?find_desc='+term+'&find_loc='+location+'&start='+str(num*10)\n",
    "    with urllib.request.urlopen(query) as url:\n",
    "        contents = url.read()\n",
    "    #contents = urllib.urlopen(query).read()\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    #print(soup)\n",
    "    business_url = []\n",
    "    for result in soup.findAll('a',{'class':'biz-name js-analytics-click'}):\n",
    "        business_url.append(\"http://www.yelp.com\" + result['href'])\n",
    "    return business_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of all locations in NYC\n",
    "searchLocations = ['Alphabet_City','Battery_Park','Chelsea','Chinatown','Civic_Center','East_Harlem','East_Village','Financial_District','Flatiron','Gramercy','Greenwich_Village','Harlem','Hell\\'s_Kitchen','Inwood','Kips_Bay','Koreatown','Little_Italy','Lower_East_Side','Manhattan_Valley','Marble_Hill','Meatpacking_District','Midtown_East','Midtown_West','Morningside_Heights','Murray_Hill','NoHo','Nolita','Roosevelt_Island','SoHo','South_Street_Seaport','South_Village','Stuyvesant_Town','TriBeCa','Two_Bridges','Union_Square','Upper_East_Side','Upper_West_Side','Washington_Heights','West_Village']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code takes about 20 minutes to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get NYC Japanese restaurant info\n",
    "max_num = 5\n",
    "urls_set = []\n",
    "for i, loc in enumerate(searchLocations):\n",
    "    # now run for loop with fix location and food type and append urls \n",
    "    # page ranking is based on relevance ranked by yelp\n",
    "    for num in range(0,max_num):\n",
    "        urls = get_urls_from_search(\"Japanese restaurants\",loc, num)\n",
    "        urls = urls[1:] # 0th link is irrelavant\n",
    "        # len(urls)=0 if the starting page number exceed the maximum possible\n",
    "        if (len(urls) ==0):\n",
    "            break\n",
    "        else:\n",
    "            for i in range(0,len(urls)-1):\n",
    "                urls_set.append(urls[i])\n",
    "                \n",
    "    # Delays to help reduce queries and reduce the possibility of IP Ban            \n",
    "    time.sleep(5)\n",
    "\n",
    "#check urls_set lenght\n",
    "print(len(urls_set))\n",
    "urls_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_urls_set = pd.DataFrame(urls_set)\n",
    "url = pd_urls_set.drop_duplicates().values.tolist() # drop duplicates to improve efficiency\n",
    "urls = [i[0] for i in url]\n",
    "# check number of restaurants pages we need to scrape\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It takes the scrape function about 20-30 seconds to scrape each webpage**\n",
    "\n",
    "**So, the following code takes about (20$\\times$ len(urls)) seconds to run**\n",
    "\n",
    "e.g. if we have 1000 urls to run then the following code takes about 6 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# header contains the info we want to scrape\n",
    "header=['restaurant_name', 'retaurant_address', 'restaurant_zipcode', 'restaurant_reviewcount', 'restaurant_rating', 'restaurant_neighobrhood', 'Hygiene_score', 'price_range', 'Liked by Vegetarians', 'Takes Reservations', 'Delivery', 'Take-out', 'Accepts Credit Cards', 'Accepts Bitcoin', 'Parking', 'Bike Parking', 'Wheelchair Accessible', 'Good for Kids', 'Good for Groups', 'Attire', 'Noise Level', 'Alcohol', 'Happy Hour', 'Outdoor Seating', 'Wi-Fi', 'Has TV', 'Dogs Allowed', 'Waiter Service', 'Caters', 'Category', 'Has Soy-free Options', 'Has Dairy-free Options', 'Liked by Vegans', 'Has Gluten-free Options', 'Good For', 'Ambience', 'Gender Neutral Restrooms']\n",
    "info={}\n",
    "for u in urls:\n",
    "    url_dict=scrape(u)\n",
    "    for i in header:\n",
    "        if i in url_dict.keys():\n",
    "            info.setdefault(i,[]).append(url_dict[i])\n",
    "        else:\n",
    "            info.setdefault(i,[]).append('NA')\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get NYC American restaurant info\n",
    "max_num = 5\n",
    "urls_set = []\n",
    "for i, loc in enumerate(searchLocations):\n",
    "    # now run for loop with fix location and food type and append urls \n",
    "    # page ranking is based on relevance ranked by yelp\n",
    "    for num in range(0,max_num):\n",
    "        urls = get_urls_from_search(\"American restaurants\",loc, num)\n",
    "        urls = urls[1:] # 0th link is irrelavant\n",
    "        # len(urls)=0 if the starting page number exceed the maximum possible\n",
    "        if (len(urls) ==0):\n",
    "            break\n",
    "        else:\n",
    "            for i in range(0,len(urls)-1):\n",
    "                urls_set.append(urls[i])\n",
    "                \n",
    "    # Delays to help reduce queries and reduce the possibility of IP Ban            \n",
    "    time.sleep(5)\n",
    "print(len(urls_set))\n",
    "urls_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_urls_set = pd.DataFrame(urls_set)\n",
    "url = pd_urls_set.drop_duplicates().values.tolist() # drop duplicates to improve efficiency\n",
    "urls = [i[0] for i in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header=['restaurant_name', 'retaurant_address', 'restaurant_zipcode', 'restaurant_reviewcount', 'restaurant_rating', 'restaurant_neighobrhood', 'Hygiene_score', 'price_range', 'Liked by Vegetarians', 'Takes Reservations', 'Delivery', 'Take-out', 'Accepts Credit Cards', 'Accepts Bitcoin', 'Parking', 'Bike Parking', 'Wheelchair Accessible', 'Good for Kids', 'Good for Groups', 'Attire', 'Noise Level', 'Alcohol', 'Happy Hour', 'Outdoor Seating', 'Wi-Fi', 'Has TV', 'Dogs Allowed', 'Waiter Service', 'Caters', 'Category', 'Has Soy-free Options', 'Has Dairy-free Options', 'Liked by Vegans', 'Has Gluten-free Options', 'Good For', 'Ambience', 'Gender Neutral Restrooms']\n",
    "info={}\n",
    "for u in urls_set:\n",
    "    url_dict=scrape(u)\n",
    "    for i in header:\n",
    "        if i in url_dict.keys():\n",
    "            info.setdefault(i,[]).append(url_dict[i])\n",
    "        else:\n",
    "            info.setdefault(i,[]).append('NA')\n",
    "    # Delays to help reduce queries and reduce the possibility of IP Ban\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(info)\n",
    "df.to_csv('American_Restaurant.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On our local machines, we generate csv by replacing the term with:\n",
    "Chinese, Korean, American, Indian, Japanese, Spanish, French, Italian, Greek, Thai, Mexico, Vietnamese.\n",
    "\n",
    "Both \"get_urls_from_search\" and \"scrape\" are easy to get IP ban from Yelp, so we need to change IP to get all the above cuisine types, which can be very time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
